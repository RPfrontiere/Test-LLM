import os
from promptflow.core import AzureOpenAIModelConfiguration
from promptflow.evals.evaluators import RelevanceEvaluator
from promptflow.evals.evaluators import GroundednessEvaluator
from promptflow.evals.evaluators import CoherenceEvaluator
from promptflow.evals.evaluators import FluencyEvaluator
from promptflow.evals.evaluators import F1ScoreEvaluator


def model_configuration_test():
    # Initialize Azure OpenAI Connection with your environment variables
    model_config = AzureOpenAIModelConfiguration(
        azure_endpoint=os.environ.get("AZURE_OPENAI_ENDPOINT"),
        api_key=os.environ.get("AZURE_OPENAI_API_KEY"),
        azure_deployment=os.environ.get("AZURE_OPENAI_DEPLOYMENT"),
        api_version=os.environ.get("AZURE_OPENAI_API_VERSION"),
    )
    return model_config


#STANDALONE DEFINITION OF EVALUATORS

def relevance_evaluation_test(model_config, answer, context, question):
    # Initialzing Relevance Evaluator: Measures the extent to which the model's generated responses
    # are pertinent and directly related to the given questions with a score 1-5 (1 means bad and 5 means good).
    relevance_eval = RelevanceEvaluator(model_config)
    # Running Relevance Evaluator on single input row
    relevance_score = relevance_eval(
        answer= answer,
        context= context,
        question= question,
    )
    print(relevance_score)
    return relevance_score


def groundness_evaluation_test(model_config, answer, context):
    # Initialzing Groundness Evaluator: Measures how well the model's generated answers align with information
    # from the source data (user-defined context) with a score 1-5 (1 means ungrounded and 5 means grounded).
    groundness_eval = GroundednessEvaluator(model_config)
    # Running Groundness Evaluator on single input row
    groundness_score = groundness_eval(
        answer=answer,
        context=context,
    )
    print(groundness_score)
    return groundness_score

def coherence_evaluation_test(model_config, answer, question):
    # Initialzing Coherence Evaluator: Measures how well the language model can produce output that flows smoothly,
    # reads naturally, and resembles human-like language with a score 1-5 (1 means bad and 5 means good).
    coherence_eval = CoherenceEvaluator(model_config)
    # Running Coherence Evaluator on single input row
    coherence_score = coherence_eval(
        answer= answer,
        question= question,
    )
    print(coherence_score)
    return coherence_score

def fluency_evaluation_test(model_config, answer, question):
    # Initialzing Fluency Evaluator: Measures the grammatical proficiency of a generative AI's predicted answer
    #  with a score 1-5 (1 means bad and 5 means good).
    fluency_eval = FluencyEvaluator(model_config)
    # Running Fluency Evaluator on single input row
    fluency_score = fluency_eval(
        answer= answer,
        question=question,
    )
    print(fluency_score)
    return fluency_score

def f1_evaluation_test(model_config, answer, ground_truth):
    # Initialzing F1-Score Evaluator: Measures the ratio of the number of shared words between the model generation and 
    # the ground truth answers (Float [0-1]).
    f1_eval = F1ScoreEvaluator(model_config)
    # Running Fluency Evaluator on single input row
    f1_score = f1_eval(
        answer= answer,
        ground_truth= ground_truth,
    )
    print(f1_score)
    return f1_score

def exact_match(answer, expected_answer):
    # check if the expected_answer and the answer generated by via AI are identical (True or False return value)
    return answer == expected_answer

